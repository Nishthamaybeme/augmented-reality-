{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishthamaybeme/augmented-reality-/blob/main/vit_butterfly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cnG4oqltNgr",
        "outputId": "7b53c991-4fb2-4a8f-9670-3ff60e9f2dd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw1A6sFHtc5S",
        "outputId": "002bd0c3-c9c0-46fc-f81e-4d77d9155cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset contains 832 samples.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "class LeedsButterflyDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_dir = os.path.join(root_dir, 'images')\n",
        "        self.image_files = [f for f in os.listdir(self.image_dir) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Fix the label extraction: get first 3 digits as category ID\n",
        "        category_id = int(self.image_files[idx][:3])  # Extract the category ID (first 3 digits)\n",
        "        label = category_id - 1  # Category IDs start from 1, but labels start from 0\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Set dataset path\n",
        "DATASET_PATH = '/content/drive/MyDrive/crime game/leedsbutterfly'\n",
        "\n",
        "# Define transformation (resize to 224x224 for ViT input)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images for ViT\n",
        "    transforms.ToTensor(),          # Convert to PyTorch Tensor\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset = LeedsButterflyDataset(DATASET_PATH, transform=transform)\n",
        "\n",
        "# Create data loader\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Check if dataset is loaded correctly\n",
        "print(f\"Dataset contains {len(dataset)} samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wp1gjeXuA-o",
        "outputId": "d9ea65d7-301f-442b-b17c-fa9b2870c1eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pre-trained ViT model and feature extractor\n",
        "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=10)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Load the feature extractor for ViT\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0ml6249lgXx7",
        "outputId": "24cc8d62-0168-4e3e-97f1-55801a82c6dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 1.4488209623556871\n",
            "Epoch [2/5], Loss: 0.45283622581225175\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "epochs = 5  # Number of epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images).logits  # Logits are the raw predictions\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwgO97TpuJVK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the directory path\n",
        "save_directory = '/content/drive/My Drive/new_model_directory'\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# Define the full save path for the model\n",
        "save_path = os.path.join(save_directory, 'vit_butterfly_model1.pth')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xn6y8Nozdon"
      },
      "outputs": [],
      "source": [
        "# Re-initialize the model (same architecture as before)\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=10)\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/new_model_directory/vit_butterfly_model.pth'))\n",
        "\n",
        "# Move the model to the device (GPU or CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFxDrvBWzkZW",
        "outputId": "df16c0dc-7b9e-4c90-f983-3437374826a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: 9\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Define the image preprocessing pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # Resize to the input size of ViT\n",
        "    transforms.ToTensor(),           # Convert image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize to the ImageNet mean and std\n",
        "])\n",
        "\n",
        "# Load a new image\n",
        "image_path = '/content/drive/MyDrive/crime game/leedsbutterfly/download (3).jpg'  # Replace with your image path\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Apply the preprocessing transform\n",
        "image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Move the image to the device\n",
        "image = image.to(device)\n",
        "\n",
        "# Make the prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(image).logits\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Output the predicted class label\n",
        "print(f\"Predicted class: {predicted.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiQih4-Shz0-",
        "outputId": "8ba3f38d-a4b4-4a34-8c77-ef174dd33b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted butterfly species:\n",
            "Scientific Name: Vanessa cardui\n",
            "Common Name: Painted Lady\n"
          ]
        }
      ],
      "source": [
        "# Map class indices to butterfly species (scientific and common names)\n",
        "class_names = {\n",
        "    0: ('Danaus plexippus', 'Monarch Butterfly'),\n",
        "    1: ('Heliconius charitonius', 'Zebra Longwing'),\n",
        "    2: ('Heliconius erato', 'Red Postman'),\n",
        "    3: ('Junonia coenia', 'Common Buckeye'),\n",
        "    4: ('Lycaena phlaeas', 'Small Copper'),\n",
        "    5: ('Nymphalis antiopa', 'Mourning Cloak'),\n",
        "    6: ('Papilio cresphontes', 'Giant Swallowtail'),\n",
        "    7: ('Pieris rapae', 'Cabbage White'),\n",
        "    8: ('Vanessa atalanta', 'Red Admiral'),\n",
        "    9: ('Vanessa cardui', 'Painted Lady')\n",
        "}\n",
        "\n",
        "# Get the predicted class index (for example, class index 3)\n",
        "predicted_class_idx = predicted.item()\n",
        "\n",
        "# Get the scientific and common names for the predicted class\n",
        "predicted_scientific_name, predicted_common_name = class_names[predicted_class_idx]\n",
        "\n",
        "# Print both names\n",
        "print(f\"Predicted butterfly species:\")\n",
        "print(f\"Scientific Name: {predicted_scientific_name}\")\n",
        "print(f\"Common Name: {predicted_common_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXLqWVH6h26a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPtMh0/YLNCRuxnl/aQcm5Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}